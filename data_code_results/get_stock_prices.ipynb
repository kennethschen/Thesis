{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x21abeaa6110>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SQL db creation\n",
    "conn = sqlite3.connect('stocks_data/sp500.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "# create tables\n",
    "c.execute('CREATE TABLE IF NOT EXISTS dimension_table (`symbol` varchar(255) PRIMARY KEY, `name` varchar(255) NOT NULL, `sector` varchar(255) NOT NULL)')\n",
    "c.execute('CREATE TABLE IF NOT EXISTS fact_table (`symbol` varchar(255) NOT NULL, `date` date NOT NULL, `open` float NOT NULL, `high` float NOT NULL, `low` float NOT NULL, `close` float NOT NULL, `adj_close` float NOT NULL, `volume` float NOT NULL, PRIMARY KEY (symbol, date))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S&P 500 symbol, name, and sector from https://datahub.io/core/s-and-p-500-companies#resource-constituents\n",
    "info = pd.read_csv('sp500_info/constituents_csv.csv')\n",
    "\n",
    "# persist\n",
    "info.to_sql('dimension_table', con=conn, if_exists='replace', index=False)\n",
    "\n",
    "# tickers\n",
    "tickers = info['Symbol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://github.com/c0redumb/yahoo_quote_download\n",
    "\n",
    "# To make print working for Python2/3\n",
    "from __future__ import print_function\n",
    "\n",
    "# Use six to import urllib so it is working for Python2/3\n",
    "from six.moves import urllib\n",
    "# If you don't want to use six, please comment out the line above\n",
    "# and use the line below instead (for Python3 only).\n",
    "#import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "'''\n",
    "Starting on May 2017, Yahoo financial has terminated its service on\n",
    "the well used EOD data download without warning. This is confirmed\n",
    "by Yahoo employee in forum posts.\n",
    "Yahoo financial EOD data, however, still works on Yahoo financial pages.\n",
    "These download links uses a \"crumb\" for authentication with a cookie \"B\".\n",
    "This code is provided to obtain such matching cookie and crumb.\n",
    "'''\n",
    "\n",
    "# Build the cookie handler\n",
    "cookier = urllib.request.HTTPCookieProcessor()\n",
    "opener = urllib.request.build_opener(cookier)\n",
    "urllib.request.install_opener(opener)\n",
    "\n",
    "# Cookie and corresponding crumb\n",
    "_cookie = None\n",
    "_crumb = None\n",
    "\n",
    "# Headers to fake a user agent\n",
    "_headers={\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; U; Linux i686) Gecko/20071127 Firefox/2.0.0.11'\n",
    "}\n",
    "\n",
    "def _get_cookie_crumb():\n",
    "    '''\n",
    "    This function perform a query and extract the matching cookie and crumb.\n",
    "    '''\n",
    "\n",
    "    # Perform a Yahoo financial lookup on SP500\n",
    "    req = urllib.request.Request('https://finance.yahoo.com/quote/^GSPC', headers=_headers)\n",
    "    f = urllib.request.urlopen(req)\n",
    "    alines = f.read().decode('utf-8')\n",
    "\n",
    "    # Extract the crumb from the response\n",
    "    global _crumb\n",
    "    cs = alines.find('CrumbStore')\n",
    "    cr = alines.find('crumb', cs + 10)\n",
    "    cl = alines.find(':', cr + 5)\n",
    "    q1 = alines.find('\"', cl + 1)\n",
    "    q2 = alines.find('\"', q1 + 1)\n",
    "    crumb = alines[q1 + 1:q2]\n",
    "    _crumb = crumb\n",
    "\n",
    "    # Extract the cookie from cookiejar\n",
    "    global cookier, _cookie\n",
    "    for c in cookier.cookiejar:\n",
    "        if c.domain != '.yahoo.com':\n",
    "            continue\n",
    "        if c.name != 'B':\n",
    "            continue\n",
    "        _cookie = c.value\n",
    "\n",
    "    # Print the cookie and crumb\n",
    "    #print('Cookie:', _cookie)\n",
    "    #print('Crumb:', _crumb)\n",
    "\n",
    "def load_yahoo_quote(ticker, begindate, enddate, info = 'quote', format_output = 'list'):\n",
    "    '''\n",
    "    This function load the corresponding history/divident/split from Yahoo.\n",
    "    '''\n",
    "    # Check to make sure that the cookie and crumb has been loaded\n",
    "    global _cookie, _crumb\n",
    "    if _cookie == None or _crumb == None:\n",
    "        _get_cookie_crumb()\n",
    "\n",
    "    # Prepare the parameters and the URL\n",
    "    tb = time.mktime((int(begindate[0:4]), int(begindate[4:6]), int(begindate[6:8]), 4, 0, 0, 0, 0, 0))\n",
    "    te = time.mktime((int(enddate[0:4]), int(enddate[4:6]), int(enddate[6:8]), 18, 0, 0, 0, 0, 0))\n",
    "\n",
    "    param = dict()\n",
    "    param['period1'] = int(tb)\n",
    "    param['period2'] = int(te)\n",
    "    param['interval'] = '1d'\n",
    "    if info == 'quote':\n",
    "        param['events'] = 'history'\n",
    "    elif info == 'dividend':\n",
    "        param['events'] = 'div'\n",
    "    elif info == 'split':\n",
    "        param['events'] = 'split'\n",
    "    param['crumb'] = _crumb\n",
    "    params = urllib.parse.urlencode(param)\n",
    "    url = 'https://query1.finance.yahoo.com/v7/finance/download/{}?{}'.format(ticker, params)\n",
    "    #print(url)\n",
    "    req = urllib.request.Request(url, headers=_headers)\n",
    "\n",
    "    # Perform the query\n",
    "    # There is no need to enter the cookie here, as it is automatically handled by opener\n",
    "    f = urllib.request.urlopen(req)\n",
    "    alines = f.read().decode('utf-8')\n",
    "    #print(alines)\n",
    "    if format_output == 'list':\n",
    "        return alines.split('\\n')\n",
    "\n",
    "    if format_output == 'dataframe':\n",
    "        nested_alines = [line.split(',') for line in alines.split('\\n')[1:]]\n",
    "        cols = alines.split('\\n')[0].split(',')\n",
    "        adf = pd.DataFrame.from_records(nested_alines[:-1], columns=cols)\n",
    "        return adf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▊                                                                               | 11/505 [00:11<08:26,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AET\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▏                                                                         | 45/505 [00:39<05:40,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANDV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|███████████▏                                                                     | 70/505 [01:05<07:19,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRK.B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█████████████▎                                                                   | 83/505 [01:16<05:54,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BF.B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█████████████▋                                                                   | 85/505 [01:17<04:47,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|█████████████████████                                                           | 133/505 [01:57<04:11,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSRA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|████████████████████████▌                                                       | 155/505 [02:19<08:06,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|██████████████████████████▉                                                     | 170/505 [02:34<04:29,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVHC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|████████████████████████████▉                                                   | 183/505 [02:48<07:05,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESRX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|█████████████████████████████████▎                                              | 210/505 [03:12<05:36,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GGP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|████████████████████████████████████████████▋                                   | 282/505 [04:35<04:03,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LUK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████████████████████████████████████████████████▏                             | 317/505 [05:12<03:51,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|██████████████████████████████████████████████████████████▉                     | 372/505 [06:09<02:11,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████████████████████████████████████████████████████████████                 | 398/505 [06:36<01:45,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|██████████████████████████████████████████████████████████████████████▍         | 445/505 [07:31<01:08,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TWX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|██████████████████████████████████████████████████████████████████████████████▎ | 494/505 [08:26<00:12,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WYN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|███████████████████████████████████████████████████████████████████████████████ | 499/505 [08:32<00:07,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 505/505 [08:38<00:00,  1.06s/it]\n"
     ]
    }
   ],
   "source": [
    "# iterate over tickers\n",
    "for ticker in tqdm(tickers):\n",
    "    \n",
    "    # load and format data\n",
    "    try:\n",
    "        rawdata = load_yahoo_quote(ticker, '19900101', '20190101')\n",
    "    except:\n",
    "        print(ticker)\n",
    "        continue\n",
    "    names = rawdata[0].lower().replace(' ', '_').split(',')\n",
    "    data = pd.DataFrame(rawdata[1:-1])\n",
    "    df = pd.DataFrame(data[0].str.split(',').tolist(), columns = names)\n",
    "    df.insert(0, 'symbol', ticker)\n",
    "    \n",
    "    # persist\n",
    "    df.to_sql('fact_table', con=conn, if_exists='append', index=False)\n",
    "    conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tesla\n",
    "rawdata = load_yahoo_quote('tsla', '19900101', '20190101')\n",
    "names = rawdata[0].lower().replace(' ', '_').split(',')\n",
    "data = pd.DataFrame(rawdata[1:-1])\n",
    "df = pd.DataFrame(data[0].str.split(',').tolist(), columns = names)\n",
    "df.insert(0, 'symbol', 'TSLA')\n",
    "\n",
    "df.to_csv('stocks_data/tesla.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
